{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\abelc\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\abelc\\anaconda3\\lib\\site-packages (from requests)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\abelc\\anaconda3\\lib\\site-packages (from requests)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\abelc\\anaconda3\\lib\\site-packages (from requests)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abelc\\anaconda3\\lib\\site-packages (from requests)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Load the required packages\n",
    "!pip install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\abelc\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\abelc\\anaconda3\\lib\\site-packages (from sklearn)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: BeautifulSoup4 in c:\\users\\abelc\\anaconda3\\lib\\site-packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abelc\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\abelc\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# Scrap postings from the web\n",
    "texts = []\n",
    "for i in range(0,1000,10): # Here we are cyclying through 100 pages of indeed job resources\n",
    "    soup = BeautifulSoup(requests.get('http://www.indeed.com/jobs?q=data+scientist&start='+str(i)).text)\n",
    "    texts += [a.text for a in soup.findAll('span', {'class':'summary'})]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            Deep understanding of machine learning techniques such as optimization, deep learning, reinforcement learning, recurrent neural networks (RNN), dynamic bayesian...\n"
     ]
    }
   ],
   "source": [
    "# Lets go ahead and print the first job description of a data analyst\n",
    "print (texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now do basic counts of the job description using count vectorizer\n",
    "vect = CountVectorizer(ngram_range=(1,2), stop_words='english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now fit and learn the vocabulary in the large collection of text we have gathered\n",
    "matrix = vect.fit_transform(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9128\n"
     ]
    }
   ],
   "source": [
    "# For kicks and giggles lets find out how many features are in the corpus\n",
    "print (len(vect.get_feature_names())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will now sort the frequency of words from smallest to largest\n",
    "freqs = [(word, matrix.getcol(idx).sum()) for word, idx in vect.vocabulary_.items()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 2265\n",
      "learning 769\n",
      "machine 529\n",
      "machine learning 513\n",
      "scientist 446\n",
      "data scientist 431\n",
      "analytics 327\n",
      "mining 305\n",
      "deep 295\n",
      "experience 274\n",
      "statistical 220\n",
      "analysis 218\n",
      "ml 211\n",
      "team 203\n",
      "techniques 195\n",
      "data mining 195\n",
      "scientists 194\n",
      "technical 188\n",
      "visualization 181\n",
      "modeling 167\n",
      "data scientists 166\n",
      "statistics 165\n",
      "scientific 157\n",
      "knowledge 152\n",
      "data visualization 148\n"
     ]
    }
   ],
   "source": [
    "for phrase, times in sorted (freqs, key = lambda x: -x[1])[:25]:\n",
    "    print (phrase, times)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1539\n"
     ]
    }
   ],
   "source": [
    "print (len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some words dont make sense but if you are familiar with the data science terminologies you will get pretty good idea the skills in demand for data scientist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
